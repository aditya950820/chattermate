version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend.prod
    ports:
      - "8001:8000"
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/chattermate
      - REDIS_ENABLED=true
      - REDIS_URL=redis://redis:6379/0
      - WORKERS=1
      - TIMEOUT=120
      # Optional: override these at deploy time in Coolify if needed
      - FRONTEND_URL=${FRONTEND_URL:-http://localhost:8080}
      - VITE_WIDGET_URL=${VITE_WIDGET_URL:-http://localhost:8080}
      - APP_BASE_URL=${APP_BASE_URL:-http://localhost:8001}
      # HuggingFace/PyTorch caches
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
      - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface/sentence_transformers
      - HF_HUB_CACHE=/app/.cache/huggingface/hub
      - HF_HUB_DISABLE_TELEMETRY=1
      - TORCH_HOME=/app/.cache/torch
      - PYTORCH_TRANSFORMERS_CACHE=/app/.cache/pytorch_transformers
      # Embedding safety in Docker
      - EMBEDDING_MAX_WORKERS=4
      - KB_MAX_WORKERS=4
      - EMBEDDING_SINGLE_THREADED=true
      - EMBEDDING_SEQUENTIAL_FALLBACK=true
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-X", "GET", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - backend_data:/app/uploads
      - ./config:/app/config
      - huggingface_cache:/app/.cache/huggingface
      - torch_cache:/app/.cache/torch
    restart: unless-stopped
    networks:
      - app-network

  knowledge_processor:
    build:
      context: .
      dockerfile: Dockerfile.backend.prod
    command: python -m app.workers.knowledge_processor
    environment:
      - ENVIRONMENT=production
      - DATABASE_URL=postgresql+psycopg://postgres:postgres@db:5432/chattermate
      - REDIS_ENABLED=true
      - REDIS_URL=redis://redis:6379/0
      - PYTHONUNBUFFERED=1
      # HuggingFace/PyTorch caches
      - HF_HOME=/app/.cache/huggingface
      - TRANSFORMERS_CACHE=/app/.cache/huggingface/transformers
      - SENTENCE_TRANSFORMERS_HOME=/app/.cache/huggingface/sentence_transformers
      - HF_HUB_CACHE=/app/.cache/huggingface/hub
      - HF_HUB_DISABLE_TELEMETRY=1
      - TORCH_HOME=/app/.cache/torch
      - PYTORCH_TRANSFORMERS_CACHE=/app/.cache/pytorch_transformers
      # Embedding safety in Docker
      - EMBEDDING_MAX_WORKERS=2
      - KB_MAX_WORKERS=2
      - EMBEDDING_SINGLE_THREADED=true
      - EMBEDDING_SEQUENTIAL_FALLBACK=true
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - backend_data:/app/uploads
      - ./config:/app/config
      - huggingface_cache:/app/.cache/huggingface
      - torch_cache:/app/.cache/torch
    restart: always
    networks:
      - app-network

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend.prod
    ports:
      - "8080:80"
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - app-network

  db:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=chattermate
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  backend_data:
  huggingface_cache:
  torch_cache: 